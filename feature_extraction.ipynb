{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from keras import Model\n",
    "from keras.src.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from keras.src.utils import img_to_array\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, element_at, split, udf, monotonically_increasing_id\n",
    "from pyspark.sql.types import ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"s3://xavier-project-8-bucket/Test/\"\n",
    "RESULTS_PATH = \"s3://xavier-project-8-bucket/results/\"\n",
    "PCA_RESULTS_PATH = \"s3://xavier-project-8-bucket/results_pca/\"\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName('project-8-spark-application')\n",
    "         .config(\"spark.executor.memory\", \"15g\") # C5.xlarge instances has 16gb of memory\n",
    "         .config(\"spark.driver.memory\", \"15g\")\n",
    "         .config(\"spark.executor.cores\", \"4\")  # C5.xlarge instanced have 4 vcpus\n",
    "         .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\n",
    "         .getOrCreate()\n",
    "         )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed\n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(broadcasted_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    # array_to_vector = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "    # spark_df = features_df.withColumn(\"features\", array_to_vector(\"features\"))\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches. This amortizes the overhead of loading big models.\n",
    "    model = load_model()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_array(vector):\n",
    "    \"\"\"Convert a spark vector to a numpy array\"\"\"\n",
    "    return vector.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_pca(spark_df, n_components=10, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Implements incremental PCA using batched processing.\n",
    "    Args:\n",
    "        spark_df: Input Spark DataFrame with features column containing vectors\n",
    "        n_components: Number of PCA components to keep\n",
    "        batch_size: Size of batches for incremental processing\n",
    "    Returns:\n",
    "        Transformed DataFrame and explained variance ratios\n",
    "    \"\"\"\n",
    "    # First, count total rows to determine number of batches\n",
    "    # First, optimize partitioning based on data size and batch size\n",
    "    total_rows = spark_df.count()\n",
    "    num_partitions = max(total_rows // batch_size, 1)  # At least one partition\n",
    "\n",
    "    # Add unique IDs and repartition the data\n",
    "    df_partitioned = (spark_df\n",
    "                      .withColumn(\"id\", monotonically_increasing_id())\n",
    "                      .repartition(num_partitions)\n",
    "                      .cache())  # Cache the repartitioned data\n",
    "\n",
    "    # Process each partition in parallel\n",
    "    def process_partition(iterator):\n",
    "        \"\"\"Process a partition of data and return local statistics\"\"\"\n",
    "        partition_data = []\n",
    "        for row in iterator:\n",
    "            partition_data.append(vector_to_array(row.features))\n",
    "        if not partition_data:\n",
    "            return\n",
    "        partition_data = np.array(partition_data)\n",
    "\n",
    "        # Calculate local statistics\n",
    "        local_sum = partition_data.sum(axis=0)\n",
    "        local_n_samples = len(partition_data)\n",
    "\n",
    "        # Calculate local SVD\n",
    "        local_centered = partition_data - partition_data.mean(axis=0)\n",
    "        U, S, Vt = np.linalg.svd(local_centered, full_matrices=False)\n",
    "\n",
    "        # Return local statistics\n",
    "        yield (local_n_samples, local_sum, S[:n_components], Vt[:n_components])\n",
    "\n",
    "    # Collect local statistics from all partitions\n",
    "    local_stats = df_partitioned.rdd.mapPartitions(process_partition).collect()\n",
    "\n",
    "    # Combine statistics from all partitions\n",
    "    total_n_samples = sum(stat[0] for stat in local_stats)\n",
    "    mean_sum = sum(stat[1] for stat in local_stats)\n",
    "    global_mean = mean_sum / total_n_samples\n",
    "\n",
    "    # Combine components using SVD update\n",
    "    all_components = []\n",
    "    all_singular_values = []\n",
    "    for _, _, S, Vt in local_stats:\n",
    "        all_components.append(S.reshape(-1, 1) * Vt)\n",
    "        all_singular_values.extend(S)\n",
    "    if all_components:\n",
    "        combined = np.vstack(all_components)\n",
    "        U_combined, S_combined, Vt_combined = np.linalg.svd(combined, full_matrices=False)\n",
    "\n",
    "        # Get final components and variance\n",
    "        components = Vt_combined[:n_components]\n",
    "        singular_values = S_combined[:n_components]\n",
    "        explained_variance = (singular_values ** 2) / (total_n_samples - 1)\n",
    "        total_var = explained_variance.sum()\n",
    "        explained_variance_ratio = explained_variance / total_var\n",
    "    else:\n",
    "        raise ValueError(\"No data was processed\")\n",
    "\n",
    "    # Create transformation function\n",
    "    def transform_vector(v):\n",
    "        v_array = vector_to_array(v)\n",
    "        transformed = np.dot(v_array - global_mean, components.T)\n",
    "        return Vectors.dense(transformed)\n",
    "\n",
    "    # Register UDF for transformation\n",
    "    transform_udf = udf(transform_vector, VectorUDT())\n",
    "\n",
    "    # Transform dataset in parallel\n",
    "    result_df = df_partitioned.withColumn(\"pca_features\", transform_udf(\"features\"))\n",
    "\n",
    "    # Cleanup cached data\n",
    "    df_partitioned.unpersist()\n",
    "    return result_df, explained_variance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Spark script.\\n\")\n",
    "\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(DATA_PATH)\n",
    "\n",
    "print(f\"Loaded {images.count()} images from {DATA_PATH}\")\n",
    "\n",
    "images = images.withColumn('label', element_at(split(images['path'], '/'), -2))\n",
    "\n",
    "# Transfer learning, we don't keep the last layer\n",
    "model = MobileNetV2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "new_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "broadcasted_weights = sc.broadcast(new_model.get_weights())\n",
    "print(\"The weights have been broadcasted.\\n\")\n",
    "\n",
    "# Featurize images and saving the results\n",
    "numbers_to_float_udf = udf(lambda x: [float(number) for number in x], ArrayType(FloatType()))\n",
    "array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "features_df = (images\n",
    "               .repartition(20)\n",
    "               .withColumn(\"features\", featurize_udf(\"content\"))\n",
    "               .withColumn(\"features\", numbers_to_float_udf(\"features\"))\n",
    "               .withColumn(\"features\", array_to_vector_udf(\"features\"))\n",
    "               .select(\"path\", \"label\", \"features\"))\n",
    "features_df.write.mode(\"overwrite\").parquet(RESULTS_PATH)\n",
    "print(f\"The features have been saved to {RESULTS_PATH}.\\n\")\n",
    "\n",
    "\n",
    "# Load the results back\n",
    "spark_df = spark.read.parquet(RESULTS_PATH)\n",
    "print(\"The results have been loaded back.\\n\")\n",
    "\n",
    "# Missing PCA step added\n",
    "pca = PCA(k=10, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(spark_df)\n",
    "df_pca = pca_model.transform(spark_df)\n",
    "\n",
    "df_pca.write.mode(\"overwrite\").parquet(PCA_RESULTS_PATH)\n",
    "\n",
    "explained_variance = pca_model.explainedVariance\n",
    "print(\"Explained variance ratio:\", explained_variance)\n",
    "\n",
    "\n",
    "# Load the results back again\n",
    "spark_pca_df = spark.read.parquet(PCA_RESULTS_PATH)\n",
    "print(\"The results with PCA have been loaded back.\\n\")\n",
    "\n",
    "print(spark_pca_df.show(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
