{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1730775316289_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-6-96.eu-west-3.compute.internal:20888/proxy/application_1730775316289_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-5-136.eu-west-3.compute.internal:8042/node/containerlogs/container_1730775316289_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from keras import Model\n",
    "from keras.src.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from keras.src.utils import img_to_array\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, element_at, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = \"s3a://xavier-project-8-bucket/Test/\"\n",
    "RESULTS_PATH = \"s3a://xavier-project-8-bucket/results/\"\n",
    "PCA_RESULTS_PATH = \"s3a://xavier-project-8-bucket/results_pca/\"\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName('project-8-spark-application')\n",
    "         .config(\"spark.executor.memory\", \"15g\") # C5.xlarge instances has 16gb of memory\n",
    "         .config(\"spark.driver.memory\", \"15g\")\n",
    "         .config(\"spark.executor.cores\", \"4\")  # C5.xlarge instanced have 4 vcpus\n",
    "         .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\n",
    "         .getOrCreate()\n",
    "         )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MISC CONFIG\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed\n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    new_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(broadcasted_weights.value)\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    \n",
    "    return preprocess_input(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    # array_to_vector = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "    # spark_df = features_df.withColumn(\"features\", array_to_vector(\"features\"))\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    \n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    \n",
    "    return pd.Series(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/yarn/usercache/livy/appcache/application_1730775316289_0002/container_1730775316289_0002_01_000001/pyspark.zip/pyspark/sql/pandas/functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details."
     ]
    }
   ],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches. This amortizes the overhead of loading big models.\n",
    "    model = load_model()\n",
    "    \n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark script.\n",
      "\n",
      "Loaded 22688 images from s3a://xavier-project-8-bucket/Test/\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "\r",
      "\u001b[1m       0/14536120\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0s/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m   16384/14536120\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 4us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m   49152/14536120\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 3us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m   81920/14536120\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 3us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m  147456/14536120\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 2us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m  212992/14536120\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 2us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m  327680/14536120\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 1us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m  475136/14536120\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 1us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m  745472/14536120\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 1us/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m 1114112/14536120\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 1us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m 1720320/14536120\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 0us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m 2695168/14536120\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 0us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m 4153344/14536120\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 0us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m 6488064/14536120\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m 8978432/14536120\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m11927552/14536120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m14467072/14536120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 0us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m14536120/14536120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "The weights have been broadcasted.\n",
      "\n",
      "The features have been saved to s3a://xavier-project-8-bucket/results/.\n",
      "\n",
      "The results have been loaded back.\n",
      "\n",
      "Explained variance ratio: [0.10140993376526122,0.08005980580315701,0.06350303180267557,0.05014785225124536,0.03535428254127414,0.029159929579133458,0.027732147239431282,0.02284890389166808,0.019859433649752786,0.019091521207050523]\n",
      "The results with PCA have been loaded back.\n",
      "\n",
      "+--------------------+--------------+--------------------+--------------------+\n",
      "|                path|         label|            features|        pca_features|\n",
      "+--------------------+--------------+--------------------+--------------------+\n",
      "|s3a://xavier-proj...|    Watermelon|[0.13241590559482...|[-2.9173956108569...|\n",
      "|s3a://xavier-proj...|Pineapple Mini|[0.0,5.1500864028...|[-4.7617026320549...|\n",
      "|s3a://xavier-proj...|Pineapple Mini|[0.0,4.9098920822...|[-3.2724395424543...|\n",
      "|s3a://xavier-proj...|Pineapple Mini|[0.0,5.0930061340...|[-3.9680935127982...|\n",
      "|s3a://xavier-proj...|    Watermelon|[0.0,0.8305122256...|[-2.7552702774916...|\n",
      "+--------------------+--------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "print(\"Starting Spark script.\\n\")\n",
    "\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(DATA_PATH)\n",
    "\n",
    "print(f\"Loaded {images.count()} images from {DATA_PATH}.\\n\")\n",
    "\n",
    "images = images.withColumn('label', element_at(split(images['path'], '/'), -2))\n",
    "\n",
    "# Transfer learning, we don't keep the last layer\n",
    "model = MobileNetV2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "new_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "broadcasted_weights = sc.broadcast(new_model.get_weights())\n",
    "print(\"The weights have been broadcasted.\\n\")\n",
    "\n",
    "# Featurize images and saving the results\n",
    "numbers_to_float_udf = udf(lambda x: [float(number) for number in x], ArrayType(FloatType()))\n",
    "array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "features_df = (images\n",
    "               .repartition(20)\n",
    "               .withColumn(\"features\", featurize_udf(\"content\"))\n",
    "               .withColumn(\"features\", numbers_to_float_udf(\"features\"))\n",
    "               .withColumn(\"features\", array_to_vector_udf(\"features\"))\n",
    "               .select(\"path\", \"label\", \"features\"))\n",
    "features_df.write.mode(\"overwrite\").parquet(RESULTS_PATH)\n",
    "print(f\"The features have been saved to {RESULTS_PATH}.\\n\")\n",
    "\n",
    "\n",
    "# Load the results back\n",
    "spark_df = spark.read.parquet(RESULTS_PATH)\n",
    "print(\"The results have been loaded back.\\n\")\n",
    "\n",
    "# Missing PCA step added\n",
    "pca = PCA(k=42, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(spark_df)\n",
    "df_pca = pca_model.transform(spark_df)\n",
    "\n",
    "# Saving df with PCA as parquet and csv\n",
    "(df_pca.write.mode(\"overwrite\")\n",
    " .parquet(PCA_RESULTS_PATH))\n",
    "(df_pca\n",
    " .withColumn(\"features\", col(\"features\").cast(\"string\"))\n",
    " .withColumn(\"pca_features\", col(\"pca_features\").cast(\"string\"))\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .format(\"csv\")\n",
    " .save(f\"{PCA_RESULTS_PATH}_csv\"))\n",
    "\n",
    "explained_variance = pca_model.explainedVariance\n",
    "# print(\"Explained variance ratio:\", explained_variance)\n",
    "print(f\"Total variance: {sum(explained_variance.toArray())}.\\n\")\n",
    "\n",
    "\n",
    "# Load the results back again\n",
    "spark_pca_df = spark.read.parquet(PCA_RESULTS_PATH)\n",
    "print(\"The results with PCA have been loaded back.\\n\")\n",
    "\n",
    "print(spark_pca_df.show(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
